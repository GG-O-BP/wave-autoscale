kind: Metric
id: telegraf_cloudwatch_aws_lambda_metrics
collector: telegraf
metadata:
  inputs:
    cloudwatch:
      - region: "{{ region }}"
        profile: "{{ profile }}"
        period: "1m"
        delay: "1m"
        interval: "1m"
        namespaces: ["AWS/Lambda"]
        metrics: # SELECT AVG(Invocations) FROM SCHEMA("AWS/Lambda", FunctionName) WHERE FunctionName = '"{{ function_name }}"'
          - names: ["Invocations"] # Tracks the number of invocations of Lambda functions. This metric represents the workload of the function. An increase in invocations indicates an increase in the function's workload, potentially requiring scale-out. For example, if the Invocations metric exceeds a certain threshold, you can perform scale-out by adding additional replicas of the Lambda function.
            statistic_include: ["sum"] # Collect the metrics with the Sum statistic.
            dimensions:
              - name: FunctionName
                value: "{{ function_name }}"
        tags:
          metric_id: telegraf_cloudwatch_lambda_metrics
      - region: "{{ region }}"
        profile: "{{ profile }}"
        period: "1m"
        delay: "1m"
        interval: "1m"
        namespaces: ["AWS/Lambda"]
        metrics: # SELECT AVG(Duration) FROM SCHEMA("AWS/Lambda", FunctionName) WHERE FunctionName = '"{{ function_name }}"'
          - names: ["Duration"] # Tracks the execution time of Lambda functions. An increase in function execution time can impact scaling decisions. Functions with longer execution times may consume more resources, potentially necessitating scale-out. Additionally, functions with long execution times can also impact throttling and Lambda costs.
            statistic_include: ["average"] # Collect the metrics with the Average statistic, or Maximum statistic.
            dimensions:
              - name: FunctionName
                value: "{{ function_name }}"
        tags:
          metric_id: telegraf_cloudwatch_lambda_metrics
      - region: "{{ region }}"
        profile: "{{ profile }}"
        period: "1m"
        delay: "1m"
        interval: "1m"
        namespaces: ["AWS/Lambda"]
        metrics: # SELECT SUM(Errors) FROM SCHEMA("AWS/Lambda", FunctionName) WHERE FunctionName = '"{{ function_name }}"'
          - names: ["Errors"] # Tracks the number of errors occurring in the function. An increase in errors may indicate performance issues in the function, necessitating scale-out or code improvements. In case of a high error rate, triggering alarms can prompt rapid corrective actions.
            statistic_include: ["sum"] # Collect the metrics with the Sum statistic.
            dimensions:
              - name: FunctionName
                value: "{{ function_name }}"
        tags:
          metric_id: telegraf_cloudwatch_lambda_metrics
      - region: "{{ region }}"
        profile: "{{ profile }}"
        period: "1m"
        delay: "1m"
        interval: "1m"
        namespaces: ["AWS/Lambda"]
        metrics: # SELECT SUM(Throttles) FROM SCHEMA("AWS/Lambda", FunctionName) WHERE FunctionName = '"{{ function_name }}"'
          - names: ["Throttles"] # Tracks the number of times the Lambda function has been throttled. Throttling occurs when the concurrent execution limit of the function is exceeded. Throttles can indicate the need for scale-out or adjustments in resource allocation for the function.
            statistic_include: ["sum"] # Collect the metrics with the Sum statistic.
            dimensions:
              - name: FunctionName
                value: "{{ function_name }}"
        tags:
          metric_id: telegraf_cloudwatch_lambda_metrics
      - region: "{{ region }}"
        profile: "{{ profile }}"
        period: "1m"
        delay: "1m"
        interval: "1m"
        namespaces: ["AWS/Lambda"]
        metrics: # SELECT MAX(Throttles) FROM SCHEMA("AWS/Lambda", FunctionName) WHERE FunctionName = '"{{ function_name }}"'
          - names: ["ConcurrentExecutions"] # Tracks the number of concurrent executions of the function. This metric represents the current load on the function, and an increase in concurrent executions may indicate the need for scale-out. Adjusting resource allocation for the Lambda function based on the Concurrent Executions metric helps maintain optimal performance.
            statistic_include: ["maximum"] # Collect the metrics with the Maximum statistic.
            dimensions:
              - name: FunctionName
                value: "{{ function_name }}"
        tags:
          metric_id: telegraf_cloudwatch_lambda_metrics
  outputs:
    wave-autoscale: {}
  agent:
    interval: "10s" # Default data collection interval for all inputs. Intervals are durations of time and can be specified for supporting settings by combining an integer value and time unit as a string value. Valid time units are ns, us (or Âµs), ms, s, m, h.
    round_interval: true # Rounds collection interval to interval ie, if interval="10s" then always collect on :00, :10, :20, etc.
    metric_batch_size: 1000 # Telegraf will send metrics to outputs in batches of at most metric_batch_size metrics. This controls the size of writes that Telegraf sends to output plugins.
    metric_buffer_limit: 10000 # Maximum number of unwritten metrics per output. Increasing this value allows for longer periods of output downtime without dropping metrics at the cost of higher maximum memory usage.
    collection_jitter: "0s" # Collection jitter is used to jitter the collection by a random interval. Each plugin will sleep for a random time within jitter before collecting. This can be used to avoid many plugins querying things like sysfs at the same time, which can have a measurable effect on the system.
    flush_interval: "1s" # Default flushing interval for all outputs. Maximum flush_interval will be flush_interval + flush_jitter.
    flush_jitter: "0s" # Default flush jitter for all outputs. This jitters the flush interval by a random amount. This is primarily to avoid large write spikes for users running a large number of telegraf instances. ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s.
    precision: "0s" # Collected metrics are rounded to the precision specified as an interval. Precision will NOT be used for service inputs. It is up to each individual service input to set the timestamp at the appropriate precision.
    debug: false # Log at debug level.
---

